            +----------------------+
            |        OS 211        |
            |  TASK 1: SCHEDULING  |
            |    DESIGN DOCUMENT   |
            +----------------------+

---- GROUP ----

>> Fill in the names and email addresses of your group members.

Rishiraj Bheenick    <rnb14@imperial.ac.uk>
Toma Alexandru       <ait15@imperial.ac.uk>
Mihai Vanea          <mv1315@imperial.ac.uk>
Arthur Mihai Niculae <an3315@imperial.ac.uk>
Fraser Price         <fp914@imperial.ac.uk>

---- PRELIMINARIES ----

>> If you have any preliminary comments on your submission, or notes for the
>> markers, please give them here.

>> Please cite any offline or online sources you consulted while
>> preparing your submission, other than the Pintos documentation, course
>> text, lecture notes, and course staff.

None
             PRIORITY SCHEDULING
             ===================

---- DATA STRUCTURES ----

>> A1: (5 marks)
>> Copy here the declaration of each new or changed `struct' or
>> `struct' member, global or static variable, `typedef', or
>> enumeration.  Identify the purpose of each in 25 words or less.

struct thread
  {
    /* Owned by thread.c. */
    tid_t tid;                          /* Thread identifier. */
    enum thread_status status;          /* Thread state. */
    char name[16];                      /* Name (for debugging purposes). */
    uint8_t *stack;                     /* Saved stack pointer. */
    int priority;                       /* Priority. */
    int base_priority;                  /* Used to reset to original priority */
    struct lock *waiting_lock;          /* The lock that the current thread is
                                           waiting for */
    struct list holding_locks;          /* List of locks that the thread holds */
    struct list_elem allelem;           /* List element for all threads list. */

    int nice;							              /* Niceness value of thread */
    int32_t recent_cpu;					        /* CPU time allocated */

    /* Shared between thread.c and synch.c. */
    struct list_elem elem;              /* List element. */

#ifdef USERPROG
    /* Owned by userprog/process.c. */
    uint32_t *pagedir;                  /* Page directory. */
#endif

    int64_t wake_up_tick;               /* To monitor of sleep_time */
    struct list_elem sleeping_thread;   /* Add to sleeping_thread_list when
                                           calling timer_sleep*/

    /* Owned by thread.c. */
    unsigned magic;                     /* Detects stack overflow. */
  };

  base_priority keeps track of the original priority of the thread before donation.
  the value of priority is reset to base_priority whenever the current thread
  releases a lock.

  waiting_lock keeps a pointer to the lock the thread is waiting for.

  holding_locks is a list of the locks that a thread holds.

/* Lock. */
struct lock
  {
    struct thread *holder;      /* Thread holding lock (for debugging). */
    struct semaphore semaphore; /* Binary semaphore controlling access. */
    struct list_elem elem;      /* Used by threads holding_locks list   */
  };

  elem is used to allow the creation of the holding_locks list that the thread holds

>> A2: (10 marks)
>> Explain the data structure used to track priority donation.
>> Give a diagram that illustrates a nested donation in your structure.

When using priority donations we create a tree-like structure for a thread.
A thread keeps a reference to the lock it is waiting for and a list of locks which
it holds, allowing us to easily deal with priority donation.

               +--------+
               |        |
Thread 1 +-----> Lock A X----+
    5          |        |    |
               +--------+    |
                             |
                             |---+ Thread 2
               +--------+    |       10
               |        |    |
Thread 3 +-----X Lock B <----+
  20           |        |
               +--------+

Thread 1 (Priority 5)  holding_locks -> [Lock A]
                       waiting_lock  -> NULL

Thread 2 (Priority 10) holding_locks -> [Lock B]
                       waiting_lock  -> Lock A

Thread 3 (Priority 20) holding_locks -> []
                       waiting_lock  -> Lock B

In the above example, Thread 3 goes to its parent lock (waiting_lock) and donates 
its priority and Thread 2 donates its effective priority (which is now 20) to Thread 1.

In the case of multiple donations we use the holding_locks to search for the
lock which blocks the highest priority thread, and donate its priority.

---- ALGORITHMS ----

>> A3: (5 marks)
>> How do you ensure that the highest priority thread waiting for
>> a lock, semaphore, or condition variable wakes up first?

When a semaphore releases a thread (i.e. when upped), we use the sema_find_max_pri_waiter()
function to iterate through the list of blocked threads and find the highest
priority thread, which will then be released.

When a lock is released, we find the maximum priority thread waiting on its semaphore using
the method above.

When a condition variable is signalled we iterate through the list of semaphores that a
condition variable holds and up the semaphore that blocks the highest priority
thread.

>> A4: (5 marks)
>> Describe the sequence of events when a call to lock_acquire()
>> causes a priority donation.  How is nested donation handled?

When lock_acquire() is called and the lock holder is not null,  we call lock_donate(). 

If nothing is waiting for the lock, then we return. lock_donate() then checks if the priority 
of the current thread is higher than the priority of the thread that holds the lock 
that it tries to acquire. If this is the case, we set the priority of the thread holding the 
lock to the current thread's priority. We then call lock_donate() on the thread holding the lock, 
therefore recursively checking if this thread is itself waiting for a lock with a lower priority.

>> A5: (5 marks)
>> Describe the sequence of events when lock_release() is called
>> on a lock that a higher-priority thread is waiting for.

When lock_release() is called, we need to pick the highest priority thread to
run; to do this we iterate through all the locks that holding thread holds.
For each lock we use sema_find_max_pri_waiter() to find the highest priority
thread waiting on this lock, and use this information to find the maximum
priority thread waiting on any lock held by the holding thread. 

After we have found the maximum priority, we check it against the holding thread's priority. 
We then set the holding thread's priority to the maximum of the max priority found in our 
search and the holding thread's base priority base priority.

Finally we set the lock holder to null and up the lock's semaphore.

---- SYNCHRONIZATION ----

>> A6: (5 marks)
>> Describe a potential race in thread_set_priority() and explain
>> how your implementation avoids it.  Can you use a lock to avoid
>> this race?

We disable interrupts in order to read atomically from the ready_list, which is the only shared
resource in the thread_set_priority() function.

We cannot use a lock here as the current thread may yield and we would have to use priority donation 
in order to release the lock.

---- RATIONALE ----

>> A7: (5 marks)
>> Why did you choose this design?  In what ways is it superior to
>> another design you considered?

Initially we considered using the ready list as it was unorderd, but decided to make it ordered 
such that the time taken to look up the max priority thread is O(1) as we can select the first element 
in the list.

For priority donations we conidered using a hash map for storing the holding_locks list for 
each thread, but instead decided to put the list on the thread struct. This had the 
advantage that it was easier to implement. Furthermore, this had the advantage that the 
lists of locks were encapsulated to each thread struct, allowing for a less error prone and
more atomic design.

              ADVANCED SCHEDULER
              ==================

---- DATA STRUCTURES ----

>> B1: (5 marks)
>> Copy here the declaration of each new or changed `struct' or
>> `struct' member, global or static variable, `typedef', or
>> enumeration.  Identify the purpose of each in 25 words or less.

Struct members in thread.h:
  struct thread
  {
  ...
  	int nice;				                 	/*Niceness value of thread*/
    int32_t recent_cpu;               /*CPU usage time allocated*/
  ...
  }

Static variables in thread.c:
  int32_t load_avg;                  /*Stores the system load average*/

---- ALGORITHMS ----

>> B2: (5 marks)
>> Suppose threads A, B, and C have nice values 0, 1, and 2.  Each
>> has a recent_cpu value of 0.  Fill in the table below showing the
>> scheduling decision and the priority and recent_cpu values for each
>> thread after each given number of timer ticks:

timer  recent_cpu    priority   thread
ticks   A   B   C   A   B   C   to run
-----  --  --  --  --  --  --   ------
0       0   0   0  63  61  58     A
4       4   0   0  62  61  59     A
8       8   0   0  61  61  59     B
12      8   4   0  61  60  59     A
16     12   4   0  60  60  59     B
20     12   8   0  60  59  59     A
24     16   8   0  59  59  59     C
28     16  12   0  59  59  58     B
32     16  12   4  59  58  58     A
36     20  12   4  58  58  58     C


>> B3: (5 marks)
>> Did any ambiguities in the scheduler specification make values
>> in the table uncertain?  If so, what rule did you use to resolve
>> them?  Does this match the behaviour of your scheduler?

The specifications only mention that the recent_cpu must be incremented
and the priority for each thread must be computed on the same timer tick.
However it does not mention in which order to do so.

In our implementation, the recent_cpu value of the current thread is
incremented first, then the priority of the threads are calculated
using this new recent_cpu value using compute_bsd_priority.

The values in the table above matches the behaviour of our scheduler.
The recent_cpu for the current thread is incremented after each tick,
and after every 4 ticks, the priorities for each thread is recomputed.
The highest priority thread will run unless multiple threads have the
same highest priority. In that case, the thread which has not run for
longest time since it was last scheduled will become the running thread.

>> B4: (5 marks)
>> How is the way you divided the cost of scheduling between code
>> inside and outside interrupt context likely to affect performance?

The advanced scheduler runs mostly inside the interrupt context since the
load_avg and recent_cpu for each thread must be computed every second using
compute_load_avg and compute_recent_cpu. If this was done outside the interrupt
context, the timer tick could interrupt the thread and the values would not be
computed at the correct time.

The thread priorities for the advanced scheduler is also computed inside the
interrupt context after every 4 ticks. This ensures that the priorities for all
the threads are recomputed before running the scheduler and hence the correct
next thread to run can be chosen. If it was done outside the interrupt context,
an interruption might cause only some of the priorities to be recomputed and
the a wrong next thread to run might be chosen.

However doing all the calculations inside the interrupt context may prevent
other interrupts from occurring, hence giving the interrupted thread less time
to run. But the values need to computed at very specific times and must not
change when while recalculating the priorities for each thread.

---- RATIONALE ----

>> B5: (5 marks)
>> Briefly critique your design, pointing out advantages and
>> disadvantages in your design choices.
Advantages:
The design is simple and easy to read. The funtions implemented are very
explicit and it is easy to understand what they do.
The fixed-point arithmetic have been implemented efficiently using macros which
improves readability of the code.

Disadvantages:
Most of the values required for the advanced scheduler are computed inside the
interrupt context and this is not a really optimal choice for reasons mentioned
in B4. So a possible improvement would be to try to move some of the
calculations outside the interrupt context without compromising the behaviour of
the advanced scheduler.

The operations on the ready_list require O(n) and sorting the list requires
O(nlogn). We could have used more efficient data stuctures instead of lists.

>> B6: (5 marks)
>> The assignment explains arithmetic for fixed-point mathematics in
>> detail, but it leaves it open to you to implement it.  Why did you
>> decide to implement it the way you did?  If you created an
>> abstraction layer for fixed-point mathematics, that is, an abstract
>> data type and/or a set of functions or macros to manipulate
>> fixed-point numbers, why did you do so?  If not, why not?S

The fixed-point arithmetics have been implemented by creating macros for
all the operations given in the specs. This enabled the calculations to
be done within the code without having to rewrite the formulae given in
the spec every time. It also improve readability of the code since the
names of the macros clearly explain what it does. The risk of mistakes in the
calculations is also minimised compared to if the operations were
directly applied each time using the formulae in the specs since they
are only written once in the file fixed-point.h. Fixing a bug in the
operations when using macros would be much easier as we only have to modify
them in fixed-point.h.

The fixed-point arithmetic may have done by implementing
functions for each operations. However, function calls would introduce a
runtime overhead. However, when using macros, the compiler replaces them
with the actual values at compile time.
